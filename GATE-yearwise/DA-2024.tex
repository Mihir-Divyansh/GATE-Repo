\iffalse
\chapter{2024}
\author{AI24BTECH11012}
\section{da}
\fi
	\item Consider performing depth-first search (DFS) on an undirected and unweighted graph $G$ starting at vertex $s$. For any vertex $u$ in $G$, $d[u]$ is the length of the shortest path from $s$ to $u$. Let $(u,v)$ be an edge in $G$ such that $d[u]<d[v]$. If the edge $(u,v)$ is explored first in the direction from $u$ to $v$ during the above DFS, then $(u,v)$ becomes a \rule{1.5cm}{0.4pt}edge.
	\begin{enumerate}
	\item tree
	\item cross
	\item back
	\item gray
	\end{enumerate}

\item For any twice differentiable function $ f : \mathbb{R} \to \mathbb{R} $, if at some $ x^* \in \mathbb{R} $, $ f'(x^*) = 0 $ and $ f''(x^*) > 0 $, then the function $ f $ necessarily has a \rule{1.5cm}{0.4pt} at $ x = x^* $.

\begin{enumerate}
    \item local minimum
    \item global minimum
    \item local maximum
    \item global maximum
\end{enumerate}

\item Match the items in \textbf{Column 1} with the items in \textbf{Column 2} in the following table:

\begin{multicols}{2}
\textbf{Column 1}
\begin{enumerate}
    \item[(p)] First In First Out
    \item[(q)] Lookup Operation
    \item[(r)] Last In First Out
\end{enumerate}

\columnbreak

\textbf{Column 2}
\begin{enumerate}
    \item[(i)] Stacks
    \item[(ii)] Queues
    \item[(iii)] Hash Tables
\end{enumerate}
\end{multicols}

\begin{enumerate}
    \item (p) $ \rightarrow $ (ii), (q) $ \rightarrow $ (iii), (r) $ \rightarrow $ (i)
    \item (p) $ \rightarrow $ (ii), (q) $ \rightarrow $ (i), (r) $ \rightarrow $ (iii)
    \item (p) $ \rightarrow $ (i), (q) $ \rightarrow $ (ii), (r) $ \rightarrow $ (iii)
    \item (p) $ \rightarrow $ (i), (q) $ \rightarrow $ (iii), (r) $ \rightarrow $ (ii)
\end{enumerate}

\item Consider the dataset with six datapoints: $ \{ (x_1, y_1), (x_2, y_2), \dots, (x_6, y_6) \} $,
where $ x_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} $, $ x_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix} $, $ x_3 = \begin{bmatrix} 0 \\ -1 \end{bmatrix} $, $ x_4 = \begin{bmatrix} -1 \\ 0 \end{bmatrix} $, $ x_5 = \begin{bmatrix} 2 \\ 2 \end{bmatrix} $, $ x_6 = \begin{bmatrix} -2 \\ -2 \end{bmatrix} $,
and the labels are given by $ y_1 = y_2 = y_5 = 1 $, and $ y_3 = y_4 = y_6 = -1 $. A hard margin linear support vector machine is trained on the above dataset.

Which \textbf{ONE} of the following sets is a possible set of support vectors?

\begin{enumerate}
    \item $ \{ x_1, x_2, x_5 \} $
    \item $ \{ x_3, x_4, x_5 \} $
    \item $ \{ x_4, x_5 \} $
    \item $ \{ x_1, x_2, x_3, x_4 \} $
\end{enumerate}

\item Match the items in \textbf{Column 1} with the items in \textbf{Column 2} in the following table:

\begin{multicols}{2}
\textbf{Column 1}
\begin{enumerate}
    \item[(p)] Principal Component Analysis
    \item[(q)] Naïve Bayes Classification
    \item[(r)] Logistic Regression
\end{enumerate}

\columnbreak

\textbf{Column 2}
\begin{enumerate}
    \item[(i)] Discriminative Model
    \item[(ii)] Dimensionality Reduction
    \item[(iii)] Generative Model
\end{enumerate}
\end{multicols}

\begin{enumerate}
    \item (p) $ \rightarrow $ (iii), (q) $ \rightarrow $ (i), (r) $ \rightarrow $ (ii)
    \item (p) $ \rightarrow $ (ii), (q) $ \rightarrow $ (i), (r) $ \rightarrow $ (iii)
    \item (p) $ \rightarrow $ (ii), (q) $ \rightarrow $ (iii), (r) $ \rightarrow $ (i)
    \item (p) $ \rightarrow $ (iii), (q) $ \rightarrow $ (ii), (r) $ \rightarrow $ (i)
\end{enumerate}

\item Euclidean distance-based $ k $-means clustering algorithm was run on a dataset of 100 points with $ k = 3 $. If the points $ \begin{bmatrix} 1 \\ 1 \end{bmatrix} $ and $ \begin{bmatrix} -1 \\ 1 \end{bmatrix} $ are both part of cluster 3, then which \textbf{ONE} of the following points is necessarily also part of cluster 3?

\begin{enumerate}
    \item $ \begin{bmatrix} 0 \\ 0 \end{bmatrix} $ \\
    \item $ \begin{bmatrix} 0 \\ 2 \end{bmatrix} $ \\
    \item $ \begin{bmatrix} 2 \\ 0 \end{bmatrix} $ \\
    \item $ \begin{bmatrix} 0 \\ 1 \end{bmatrix} $
\end{enumerate}

\item Given a dataset with $ K $ binary-valued attributes (where $ K > 2 $) for a two-class classification task, the number of parameters to be estimated for learning a naïve Bayes classifier is

\begin{enumerate}
    \item $ 2^K + 1 $
    \item $ 2K + 1 $
    \item $ 2^{K+1} + 1 $
    \item $ K^2 + 1 $
\end{enumerate}

Here's the text of the questions and options from the image:


\item Consider performing uniform hashing on an open address hash table with load factor $\alpha = \frac{n}{m} < 1$, where $n$ elements are stored in the table with $m$ slots. The expected number of probes in an unsuccessful search is at most $\frac{1}{1 - \alpha}$.

Inserting an element in this hash table requires at most \rule{1.5cm}{0.4pt} probes, on average.

\begin{enumerate}
    \item $\ln\left(\frac{1}{1 - \alpha}\right)$
    \item $\frac{1}{1 - \alpha}$
    \item $1 + \frac{\alpha}{2}$
    \item $\frac{1}{1 + \alpha}$
\end{enumerate}

\item For any binary classification dataset, let $S_B \in \mathbb{R}^{d \times d}$ and $S_W \in \mathbb{R}^{d \times d}$ be the between-class and within-class scatter (covariance) matrices, respectively. The Fisher linear discriminant is defined by $u^* \in \mathbb{R}^d$, that maximizes
\[
J(u) = \frac{u^T S_B u}{u^T S_W u}
\]
If $\lambda = J(u^*)$, $S_W$ is non-singular and $S_B \neq 0$, then $(u^*, \lambda)$ must satisfy which ONE of the following equations?

\begin{enumerate}
    \item $S_W^{-1} S_B u^* = \lambda u^*$
    \item $S_W u^* = \lambda S_B u^*$
    \item $S_B S_W u^* = \lambda u^*$
    \item $u^T u^* = \lambda^2$
\end{enumerate}

    \item Let $h_1$ and $h_2$ be two admissible heuristics used in $A^*$ search.
    
    Which ONE of the following expressions is always an admissible heuristic?
    
    \begin{enumerate}
        \item $h_1 + h_2$
        \item $h_1 \times h_2$
        \item $\frac{h_1}{h_2} \, (h_2 \neq 0)$
        \item $|h_1 - h_2|$
    \end{enumerate}

    \item Consider five random variables $U, V, W, X,$ and $Y$ whose joint distribution satisfies:
    \[
    P(U, V, W, X, Y) = P(U) P(V) P(W | U, V) P(X | W) P(Y | W)
    \]
    Which ONE of the following statements is FALSE?
    
    \begin{enumerate}
        \item $Y$ is conditionally independent of $V$ given $W$
        \item $X$ is conditionally independent of $U$ given $W$
        \item $U$ and $V$ are conditionally independent given $W$
        \item $Y$ and $X$ are conditionally independent given $W$
    \end{enumerate}

    \item Consider the following statement:

    In adversarial search, $\alpha$–$\beta$ pruning can be applied to game trees of any depth where $\alpha$ is the $(m)$ value choice we have formed so far at any choice point along the path for the MAX player and $\beta$ is the $(n)$ value choice we have formed so far at any choice point along the path for the MIN player.

    Which ONE of the following choices of $(m)$ and $(n)$ makes the above statement valid?

    \begin{enumerate}
        \item $(m) = \text{highest}, (n) = \text{highest}$
        \item $(m) = \text{lowest}, (n) = \text{highest}$
        \item $(m) = \text{highest}, (n) = \text{lowest}$
        \item $(m) = \text{lowest}, (n) = \text{lowest}$
    \end{enumerate}

    \item Consider a database that includes the following relations:

    \[
    \text{Defender}(\text{name, rating, side, goals})
    \]
    \[
    \text{Forward}(\text{name, rating, assists, goals})
    \]
    \[
    \text{Team}(\text{name, club, price})
    \]

    Which ONE of the following relational algebra expressions checks that every name occurring in Team appears in either Defender or Forward, where $\phi$ denotes the empty set?

    \begin{enumerate}
        \item $\Pi_{\text{name}}(\text{Team}) \setminus (\Pi_{\text{name}}(\text{Defender}) \cap \Pi_{\text{name}}(\text{Forward})) = \phi$
        \item $(\Pi_{\text{name}}(\text{Defender}) \cap \Pi_{\text{name}}(\text{Forward})) \setminus \Pi_{\text{name}}(\text{Team}) = \phi$
        \item $\Pi_{\text{name}}(\text{Team}) \setminus (\Pi_{\text{name}}(\text{Defender}) \cup \Pi_{\text{name}}(\text{Forward})) = \phi$
        \item $(\Pi_{\text{name}}(\text{Defender}) \cup \Pi_{\text{name}}(\text{Forward})) \setminus \Pi_{\text{name}}(\text{Team}) = \phi$
    \end{enumerate}

