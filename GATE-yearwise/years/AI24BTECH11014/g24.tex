\iffalse
\chapter{2024}
\author{AI24BTECH11014}
\section{st}
\fi

%\begin{enumerate}

\item At a single-staff checkout counter of a supermarket store, the time taken in minutes to complete the service of a customer is a random variable having probability density function given by
\begin{align}
f\brak{x} = 
\begin{cases}
    \frac{1}{10} e^{\frac{-x}{10}}  & \text{if} x \geq 0 \\
    0 & \text{otherwise}
\end{cases}
\end{align}
When you arrive at the counter, you observe that there is already one person in service. You are also informed that the person has been in service for 5 minutes. Assuming that the service times of different customers are independent of each other, the probability that your waiting time (which is the sum of your waiting time in queue and service time) is more than 15 minutes equals
\begin{enumerate}
\begin{multicols}{4}
\item $\frac{5}{2} e^{-\frac{3}{2}}$
\item $\frac{3}{2} e^{-\frac{3}{2}}$
\item $\frac{3}{2} e^{-\frac{5}{2}}$
\item $\frac{5}{2} e^{-\frac{5}{2}}$
\end{multicols}
\end{enumerate}

\item Let $X$ be a random variable having discrete uniform distribution on $\cbrak{1, 3, 5, 7, \dots, 99}$. Then $E\brak{X | X \text{is not a multiple of 15}} $ equals
\begin{enumerate}
\begin{multicols}{4}
\item $\frac{2365}{47}$
\item $\frac{2365}{50}$
\item $50$
\item $47$
\end{multicols}
\end{enumerate}

\item Let $X_1, X_2, \dots, X_n$ be independent and identically distributed random variables having $N\brak{\mu, \sigma^2)}$ distribution, where $\mu \in \mathbb{R}$ and $\sigma > 0$. Consider the following statements:
\begin{enumerate}
\item[I] If $\frac{\sqrt{5}}{\sigma(2n + 1)} \sum_{i = 1}^{n} i (X_{i} - \mu )$ has $N(0, 1)$ distribution, then $n = 2$.
\item[II] $E\left(\left(-\ln\left(\Phi \left(\frac{X_{1} - \mu }{\sigma}\right)\right)\right)^{3}\right) = 6$, where $\Phi(\cdot)$ denotes the cumulative distribution function of an $N(0, 1)$ random variable.
\end{enumerate}

Which of the above statements is/are true?

\begin{enumerate}
\begin{multicols}{4}
\item Only (I)
\item Only (II)
\item Both (I) and (II)
\item Neither (I) nor (II)
\end{multicols}
\end{enumerate}


\item Let $\{X_n\}_{n \geq 1}$ be a sequence of independent and identically distributed random variables with probability density function
\begin{align}
f(x) = \begin{cases}
e^{-(x - \theta)}, & x \geq 0,\theta \ \\
0, & \text{otherwise},
\end{cases}
\end{align}

where $\theta > 0$. Consider the following statements:
\begin{enumerate}
\item[I] $\frac{1}{n} \sum_{i=1}^n X_i$ converges in probability to $\frac{\theta + 1}{2}$ as $n \to \infty$.

\item[II] $\lim_{n \to \infty} \mathbb{E}\brak{\min\{X_1, X_2, \dots, X_n\}} = \theta $.
\end{enumerate}
Which of the above statements is/are true?

\begin{enumerate}
\begin{multicols}{4}
    \item[(A)] Only (I)
    \item[(B)] Only (II)
    \item[(C)] Both (I) and (II)
    \item[(D)] Neither (I) nor (II)
    \end{multicols}
\end{enumerate}

\item Let $\cbrak{X_{n}}_{n \geq 1}$ be a sequence of independent random variables such that $X_{n}$ has Poisson distribution with mean $\lambda_{n}$, where $\lambda_{n} = \lambda + \frac{1}{2^{n}}, n \geq 1 $, and $\lambda > 0$ is an unknown parameter. Which one of the following statements is true?
\begin{enumerate}
    \item $\frac{1}{n} \sum_{i=1}^{n} X_{i}$ is an unbiased estimator of $\lambda$
    \item $\frac{1}{n} \sum_{i=1}^{n} X_{i}$ is a consistent estimator of $\lambda$
    \item $\sum_{i=1}^{n} X_{i}$ is a consistent estimator of $\lambda$
    \item $\frac{1}{n^{2}} \sum_{i=1}^{n} X_{i}$ is an unbiased estimator of $\lambda$
\end{enumerate}

\item Let $X_1, X_2, \dots, X_{25}$  be a random sample of size 25 from a population having $N_3(\mu, \Sigma)$ distribution, where $\mu$ and non-singular $\Sigma$ are unknown parameters. Let $S = \frac{1}{24} \sum_{i=1}^{25} (X_i - \bar{X})(X_i - \bar{X})^T,$
where $\bar{X} = \frac{1}{25} \sum_{i=1}^{25} X_j$. and
$B = \myvec{ 1 && 2 && 3 \\ -1 && 0 && 1 }$ Then which one of the following statements is true?

\begin{enumerate}
    \item $24 BS{B}^{T}$ follows a Wishart distribution of order 3 with 24 degrees of freedom.
    \item $24 BS{B}^{T}$ follows a Wishart distribution of order 2 with 25 degrees of freedom.
    \item $24 BS{B}^{T}$ follows a Wishart distribution of order 2 with 24 degrees of freedom.
    \item $24 BS{B}^{T}$ follows a Wishart distribution of order 3 with 25 degrees of freedom.
\end{enumerate}

 \item Consider the simple linear regression model 
	 \begin{align}
		 Y_i = \beta_0 + \beta_1 x_i + \epsilon_i, & i = 1,2, \dots, n,
	 \end{align}
where $\beta_0, \beta_1$ are unknown parameters, and the $\epsilon_i$ 's are uncorrelated random variables with mean 0 and finite variance $\sigma^2 > 0$ . Let $\hat{\beta}_i$ be the least squares estimators of $\beta_i, i = 0, 1$. Consider the following statements:

\begin{enumerate}
    \item[(I)] A 95\% joint confidence region for $(\beta_0, \beta_1)$ is the region bounded by an ellipse.
    \item[(II)] The expression for the covariance between $\hat{\beta}_0$ and $\hat{\beta}_1$ does not involve $\sigma^2$.
\end{enumerate}

Which of the above statements is/are true?

\begin{enumerate}
\begin{multicols}{4}
    \item Only (I)
    \item Only (II)
    \item Both (I) and (II)
    \item Neither (I) nor (II)
    \end{multicols}
\end{enumerate}

\item If $f: \sbrak{-2, 2} \rightarrow \mathbb{R}$ is a continuous function, then which of the following statements 
is/are true?

\begin{enumerate}
    \item $F: \sbrak{0, 2} \rightarrow \mathbb{R} $ defined by $F\brak{x} = \int_{0}^{x} f\brak{t} \, dt$ is differentiable on $\brak{0, 2}$.
    
    \item For any $x_1, x_2, \dots, x_{10} \in \sbrak{-2, 2} $, there exists a point $x_{0} \in \sbrak{-2, 2}$ such that 
	    \begin{align}
    f\brak{x_{0}} = \frac{1}{10} \brak{ f\brak{x_1} + f\brak{x_2} + \dots + f\brak{x_{10}} }.
	    \end{align}
    
    \item $f$ is bounded on $\sbrak{-2, 2}$.
    
    \item If $f$ is differentiable at 0 and $f\brak{0} = 0$, then 
	    \begin{align}
    \lim_{x \to 0} \frac{1}{x} \brak{ f\brak{x} + f\brak{\frac{x}{2}} + \dots + f\brak{\frac{x}{10}} } = 10 f^\prime \brak{0},
	    \end{align}
    where $f^\prime$ is the derivative of $f$ at $0$.
\end{enumerate}



\item Let $A$ be an $n \times n $ real matrix. Which of the following statements is/are true?
\begin{enumerate}
\item If $A$ is a symmetric matrix such that $A + \in I_{n}$ is positive semi-definite for every $\epsilon > 0$, where $I_{n}$ is the $n \times n $  identity matrix, then A is positive semi-definite
\item If $n$ is odd, then $A - A^{T}$ is not inevitable
\item If A is a symmetric matrix such that the singular values of A are all strictly positive, then A is positive definite
\item If 1 is the only singular value of , then A is orthogonal
\end{enumerate}

\item Which of the following statements is/are true?
\begin{enumerate}
\item If $A$ is a $ 3 \times 3 $ real matrix with 3 distinct eigenvalues, then $A$ is diagonizable 
\item If $A$ is a $3 \times 3$ real matrix such that $A^{2}$ is diagonizable, then $A$ is diagonizable.
\item For real numbers a, b, c, d, e, f, if $A = \begin{bmatrix} a && b && c \\ 0 && 0 && c \\ 0 && 0 && 0 \end{bmatrix}$  is diagonizable, then $a=b=c=0$
\item For real numbers $a, b, c, d, e, f$ if $A= \begin{bmatrix} a && b && c \\ 0 && d && e \\ 0 && 0 && f \end{bmatrix}$ is diagonalizable, then $AA^{T} = A^{T}A$
\end{enumerate}

\item Let $\Omega = \cbrak{1, 2, 3, \dots }$ and $\mathcal{H}$ be the collection of all subsets of $\Omega$. Let $P$ be a probability measure on $\mathcal{H}$ such that $P\cbrak{k} = \frac{1}{2^{k}}, k \geq 1.$ Further, let $X: \Omega \rightarrow \mathbb{R}$ be defined by $X\brak{\omega} = \omega $ for all $\omega \in \Omega $. Then which of the following statements is/are true?
\begin{enumerate}
\item There exists $k \in \Omega $ such that $P\brak{X = k} < 10^{-6}$
\item $\lim_{n \to \infty} P\brak{X \geq 4 + \frac{1}{n}} = \frac{1}{16}$
\item $\lim_{n \to \infty} P\brak{ 4 + \frac{1}{n^{2}} \leq X < 5 - \frac{1}{n} }= \frac{1}{16}$
\item If $x_{n} = 3 + \frac{ \brak{-1}^{n}}{n}, n \geq 1$, then $lim_{n \to \infty}P\brak{X \leq x_{n} } = \frac{7}{8}$
\end{enumerate}


\item Let $\brak{X, Y}$ be a random vector having joint probability density function given by 
\begin{align}
f_{X, Y}\brak{x, y} = \begin{cases} \frac{3}{4} & if x^{2} \leq y \leq 1 \text{and} -1 \leq x \leq 1 \\ 0 & otherwise.
\end{cases}
\end{align}
\begin{enumerate}
\item $X$ has the same distribution as $-X$
\item $E\brak{Y | X =0} = \frac{1}{2}$
\item The correlation coefficient between $X$ and $Y$ is 0
\item $X$ and $Y$ are independent
\end{enumerate}


\item Let $\cbrak{X_{n}}_{n \geq 1}$ be a sequence of independent random variables such that the probability density function of ${X}_{n}$ is given by 
\begin{align}
f_{n}\brak{x} = \begin{cases} \frac{1}{\lambda_{n}}e^-\frac{x}{\lambda_{n}} & if x \geq 0 \\ 0 & otherwise
\end{cases} 
\end{align}
where $\lambda_{n} = 10 - \sum_{i = 1} ^{n} \frac{5}{2 ^{i-1}}.$ Which of the following statements is/are true?
\begin{enumerate}
\item $\cbrak{X_{n}}_{ n \geq 1}$ converges in distribution to the zero random variable
\item $\cbrak{X_{n}}_{ n \geq 1}$ converges in probability to the zero random variable
\item $\cbrak{X_{n}}_{ n \geq 1}$ converges in distribution to a random variable having Poisson distribution with mean 10
\item $\cbrak{X_{n}}_{ n \geq 1}$ converges in probability to a random variable having Poisson distribution with mean 10
\end{enumerate}
%\end{enumerate}
%\end{document}
